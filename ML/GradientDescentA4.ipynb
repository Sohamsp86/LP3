{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341d8127-9599-44a5-9f31-7157b2b4a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local minimum occurs at x = 3.450873173395284e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(derivative_func, initial_x, learning_rate, max_iters):\n",
    "    # Starting point\n",
    "    x = initial_x\n",
    "    history = [x]  # To store values of x for each iteration\n",
    "    \n",
    "    # Perform gradient descent\n",
    "    for i in range(max_iters):\n",
    "        grad = derivative_func(x)  # Compute gradient\n",
    "        x = x - learning_rate * grad  # Update x\n",
    "        history.append(x)  # Save history\n",
    "        # Early stopping if the gradient is near zero (local minima reached)\n",
    "        if abs(grad) < 1e-6:\n",
    "            break\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Derivative of f(x) = x^2 is f'(x) = 2x\n",
    "def derivative_func(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Parameters\n",
    "initial_x = 10  # Starting point\n",
    "learning_rate = 0.1\n",
    "max_iters = 100\n",
    "\n",
    "# Run gradient descent\n",
    "min_x, x_history = gradient_descent(derivative_func, initial_x, learning_rate, max_iters)\n",
    "\n",
    "print(\"The local minimum occurs at x =\", min_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56638682-8fa8-486f-9a68-0559280e7daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged to minimum at x = -2.9999960768114153 after 63 iterations.\n",
      "Local minimum occurs at x = -2.999995096014269\n",
      "Minimum value of function = 2.4049076048192486e-11\n"
     ]
    }
   ],
   "source": [
    "# Define the function and its derivative\n",
    "def func(x):\n",
    "    return (x + 3) ** 2\n",
    "\n",
    "def derivative(x):\n",
    "    return 2 * (x + 3)\n",
    "\n",
    "# Parameters for Gradient Descent\n",
    "x_current = 2   # Starting point\n",
    "learning_rate = 0.1\n",
    "precision = 1e-6\n",
    "max_iterations = 1000\n",
    "\n",
    "# Gradient Descent Algorithm\n",
    "for i in range(max_iterations):\n",
    "    x_new = x_current - learning_rate * derivative(x_current)\n",
    "    \n",
    "    # Check for convergence\n",
    "    if abs(x_new - x_current) < precision:\n",
    "        print(f\"Converged to minimum at x = {x_new} after {i+1} iterations.\")\n",
    "        break\n",
    "    \n",
    "    x_current = x_new\n",
    "\n",
    "# Output the result\n",
    "print(\"Local minimum occurs at x =\", x_current)\n",
    "print(\"Minimum value of function =\", func(x_current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49060b35-d8f1-4c1c-b076-b409e1edb5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
